{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curriculum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "ADDITIONAL_STOPWORDS = [\"r\", \"u\", \"2\", \"ltgt\"] # doesn't read these IN ADDITION TO not reading standard stopwords\n",
    "\n",
    "def clean(text):\n",
    "    \"A simple function to cleanup text data\"\n",
    "    \n",
    "    # assigns the WordNetLemmatizer object ('wnl')\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # declares which are stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "    \n",
    "    text = (unicodedata.normalize(\"NFKD\", text)\n",
    "           .encode('ascii', 'ignore')\n",
    "           .decode('utf-8', 'ignore')\n",
    "           .lower())\n",
    "    \n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split() # removes non-alphanumerics\n",
    "    \n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "\n",
    "df = pd.read_csv(\"spam_clean.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**^^ 'ham' = good text; 'spam' = garbage text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we take a look at the number of spam vs. the number of ham texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>0.865937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>0.134063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         n   percent\n",
       "ham   4825  0.865937\n",
       "spam   747  0.134063"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.concat([df.label.value_counts(), df.label.value_counts(normalize=True)], axis=1)\n",
    "\n",
    "labels.columns = [\"n\", \"percent\"]\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**^^ 87% of the texts are from people we know; 13% are from people we don't**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll break the data into three different pieces: \n",
    "\n",
    "1.) the words that appear in legitimate text messages;\n",
    "\n",
    "2.) the words that appear in spam text messages; and \n",
    "\n",
    "3.) All of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_words = clean(' '.join(df[df.label == 'ham'].text))\n",
    "\n",
    "spam_words = clean(' '.join(df[df.label == 'spam'].text))\n",
    "\n",
    "all_words = clean(' '.join(df.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This gives us a list of words.  Now we can transform them into a pandas series which we can use to show how often each word occurs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_freq = pd.Series(ham_words).value_counts()\n",
    "\n",
    "spam_freq = pd.Series(spam_words).value_counts()\n",
    "\n",
    "all_freq = pd.Series(all_words).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_freq.head() # shows us the most common words (in descending order) found in ham texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_freq.head() # shows us the most common words (in descending order) found in spam texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_freq.head() # # shows us the most common words (in descending order) found in all the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now combine all those frequencies into one resulting dataframe that we can work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = (pd.concat([all_freq, ham_freq, spam_freq], axis=1, sort=True)\n",
    "               .set_axis([\"all\", \"ham\", \"spam\"], axis=1, inplace=False)\n",
    "               .fillna(0).apply(lambda s: s.astype(int)))\n",
    "\n",
    "word_counts.head()\n",
    "               \n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the most frequently occuring words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**^^ 'Call' is seen 600 times, 241 times in approved texts, but 359 times in spam.  And on down the line.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are theer any words that uniquely identify a spam or ham message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([word_counts[word_counts.spam == 0].sort_values(by=\"ham\").tail(6),\n",
    "          word_counts[word_counts.ham == 0].sort_values(by=\"spam\").tail(6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**^^Looks like 'awarded,' '18,' 'guaranteed,' 'tone,' 'prize,' and 'claim' are all unique identifiers of spam, wherease the rest of the words are unique identifiers for ham.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "- I REALLY GOTTA GET BETTER AT THIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To find out the percentage of 'spam' versus 'ham:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(word_counts.assign(p_spam=word_counts.spam / word_counts['all'],\n",
    "                   p_ham=word_counts.ham / word_counts['all']).sort_values(by=\"all\")[['p_spam', 'p_ham']]\n",
    "                   .tail(20).sort_values('p_ham').plot.barh(stacked=True))\n",
    "plt.title(\"Proportion of Spam vs. Ham for the 20 most common words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**^^Nearly 80% of texts containing 'free' are spam, whereas NEXT TO NO SPAM actually cares if the person is 'ill.''**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(word_counts[(word_counts.spam > 10) & (word_counts.ham > 10)].assign(ratio=lambda df: df.spam/ (df.ham +.01))\n",
    " .sort_values(by=\"ratio\").pipe(lambda df: pd.concat([df.head(), df.tail()])))\n",
    "\n",
    "# basically, I'm saying make a df called 'word_counts' where the word count of 'spam' is more than (>) 10, and\n",
    "# the word count of 'ham' is more than (>) 10 and assign them a ratio of spam / ham +1% (.01 above).  Then,\n",
    "# take that ratio and put them into a 'pipe df.'  'Pipe' is used because we're chaining together functions and \n",
    "# anticipating a df or Series in return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip installed the following on my terminal: **python -m pip install --upgrade wordcloud**\n",
    "\n",
    "- Wordclouds allow you to id the relative frequency of different keywords using an easily digestible visual\n",
    "\n",
    "- The larger the word appears, the more frequent its appearance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "sentence = \"Mary had a little lamb, little lamb, little lamb.  Its fleece was white as snow.\"\n",
    "\n",
    "img = WordCloud(background_color=\"white\").generate(sentence) # produces a WordCloud image object\n",
    "plt.imshow(img) # displays the WordCloud image object\n",
    "plt.axis(\"off\") # turned off b/c axes aren't of any use in wordclouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So using a wordcloud for our 'Spam V. Ham' debate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cloud = WordCloud(background_color=\"white\", height=1000, width=400).generate(' '.join(all_words))\n",
    "\n",
    "ham_cloud = WordCloud(background_color=\"white\", height=600, width=800).generate(' '.join(ham_words))\n",
    "\n",
    "spam_cloud = WordCloud(background_color=\"white\", height=600, width=800).generate(' '.join(spam_words))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "axs = [plt.axes([0, 0, .5, 1]), plt.axes([.5, .5, .5, .5]), plt.axes([.5, 0, .5, .5])]\n",
    "\n",
    "\n",
    "axs[0].imshow(all_cloud)\n",
    "axs[1].imshow(ham_cloud)\n",
    "axs[2].imshow(spam_cloud)\n",
    "\n",
    "axs[0].set_title(\"All Words\")\n",
    "axs[1].set_title(\"Ham Words\")\n",
    "axs[2].set_title(\"Spam Words\")\n",
    "\n",
    "for ax in axs: ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams:\n",
    "\n",
    "- a way to combine two words together to measure the frequency an actual phrase appears\n",
    "\n",
    "- usles nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Mary had a little lamb\"\n",
    "\n",
    "bigrams = nltk.ngrams(sentence.split(), 2)\n",
    "list(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now find out which bigrams are the most frequently occurring:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_ham_bigrams = (pd.Series(nltk.ngrams(ham_words, 2)).value_counts().head(20))\n",
    "\n",
    "top_20_ham_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_ham_bigrams.sort_values().plot.barh(color=\"pink\", width=.9, figsize=(10, 6))\n",
    "\n",
    "plt.title('20 Most Frequently Occuring Ham Bigrams')\n",
    "\n",
    "plt.ylabel(\"Bigram\")\n",
    "\n",
    "plt.xlabel(\"# Occurences\")\n",
    "\n",
    "# make the labels better looking\n",
    "\n",
    "ticks, _ = plt.yticks()\n",
    "labels = top_20_ham_bigrams.reset_index()[\"index\"].apply(lambda t: t[0] + ' ' + t[1])\n",
    "_ = plt.yticks(ticks, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bigrams can be used to make a WordCloud as well:**\n",
    "\n",
    "- we supply our own values to be used to determine how big the words (or phrases) should be by using the 'generate_from_frequencies' method\n",
    "\n",
    "- the values we supply must be in the form of a dictionary where the keys are the words/phrases and the values are their corresponding numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Convert series to dictionary, and the tuples that make up the index into a single string that holds each phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_ham_bigrams.to_dict().items()} # basically k:v for k, v\n",
    "\n",
    "img = WordCloud(background_color=\"white\", width=800, height=400).generate_from_frequencies(data)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) Spam Data:\n",
    "\n",
    "        a.) Load the spam dataset\n",
    "    \n",
    "        b.) Create and explore bigrams for the spam data.  Visualize them with a word cloud.  How do they compare against the ham bigrams?\n",
    "        \n",
    "        c.) Is there any overlap in the bigrams for the spam data and the ham data?\n",
    "        \n",
    "        d.) Create and explore with trigrams (three-word phrases, or, n-grams with an n of 3) for both the spam and the ham data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the spam dataset:\n",
    "\n",
    "df = pd.read_csv(\"spam_clean.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and explore bigrams for spam:\n",
    "\n",
    "top_20_spam_bigrams = (pd.Series(nltk.ngrams(spam_words, 2)).value_counts().head(20))\n",
    "\n",
    "top_20_spam_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Spam Bigrams, then the Ham Bigrams.  Any overlap?\n",
    "\n",
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_spam_bigrams.to_dict().items()} # basically k:v for k, v\n",
    "\n",
    "img = WordCloud(background_color=\"white\", width=800, height=400).generate_from_frequencies(data)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.title(\"Spam_Bigram\")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_ham_bigrams.to_dict().items()} # basically k:v for k, v\n",
    "\n",
    "img = WordCloud(background_color=\"white\", width=800, height=400).generate_from_frequencies(data)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.title(\"Ham_Bigram\")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and explore trigrams for spam:\n",
    "\n",
    "top_20_spam_trigrams = (pd.Series(nltk.ngrams(spam_words, 3)).value_counts().head(20))\n",
    "\n",
    "top_20_spam_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore trigrams for ham words:\n",
    "\n",
    "top_20_ham_trigrams = (pd.Series(nltk.ngrams(ham_words, 3)).value_counts().head(20))\n",
    "\n",
    "top_20_ham_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Explore the blog articles using the techniques discussed in the exploration lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
