{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curriculum:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A couple of introductory notes:\n",
    "\n",
    "- Our goal with parsing text is to reduce the variability of the text itself; letters aren't letters to a computer\n",
    "\n",
    "- Entymology doesn't matter with parsing.  We're simply trying to get the statisticts of the word\n",
    "\n",
    "### Some Terms:\n",
    "\n",
    "- 'parsing' means breaking down text into smaller parts.  Literally, parsing from 'blindly' to 'blind'\n",
    "\n",
    "- 'nlt' is the Natural Language Toolkit from Python\n",
    "\n",
    "- '.encode' something to ASCII means forcing your non-ASCII characters into an ASCII format\n",
    "\n",
    "- '.decode' comes after '.encode', and transforms the encoded ASCII character back into a string for Python\n",
    "\n",
    "- 'Tokenization' - similar to the term 'parsing' - means breaking text down into discrete words / punctuation / etc.\n",
    "\n",
    "- 'Stemming' - chopping words up into their base forms - 'calls', 'called', 'calling' all have the root stem 'call'\n",
    "\n",
    "- 'Lemma' - is the root word, not root stem like 'stemming' is\n",
    "\n",
    "- 'Lemmatization' - is like 'stemming' but WAY more refined and makes the output easier to read grammatically\n",
    "\n",
    "- 'Stopword' - a word w/ little to no significance; words common in ANY document, like 'the', 'like', 'a', 'to', etc.\n",
    "\n",
    "### What follows is the Workflow to adhere to when parsing text:\n",
    "\n",
    "1.) convert all text to lowercase;\n",
    "\n",
    "2.) Remove any accented and non-ASCII characters; \n",
    "\n",
    "3.) Remove all special characters; \n",
    "\n",
    "4.) Stem (if dataset is LARGE) or Lemmatize (if I can fit on my laptop) all words;\n",
    "\n",
    "5.) Remove all stopwords; and \n",
    "\n",
    "6.) Store the clean text and the original text for use in future notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re # for replacing non-alphanumeric characters, etc\n",
    "import json\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeup.com/codeups-data-science-career-accelerator-is-here/'\n",
    "headers = {'User-Agent': 'Codeup Data Science'} # Some websites don't accept the pyhon-requests default user-agent\n",
    "response = get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running through a couple of things from 'acquire' so we can work with raw data and visualize how things get dwindled down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.text) - Shows all the text.  ALL the text\n",
    "\n",
    "# print(response.text[:400]) - prints the first 400 characters\n",
    "\n",
    "# Make a soup variable holding the response content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# soup.prettify() - to show me all the text in HTML\n",
    "\n",
    "# soup.find_all(\"a\") - to find all the anchor tags\n",
    "\n",
    "# soup.find(h1) - finds all the h1 - main header - tags\n",
    "\n",
    "# soup.get_text() - Shows me all the text from within a matching piece of soup.  In other words, the text between the tags\n",
    "\n",
    "# soup.get_text() to show all the text from within a matching piece of soup, ie: the text between the tags\n",
    "\n",
    "# soup.select(\"body a\") - shows me the bodies of all 'a' anchors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here, shite went terribly South.  My 'acquire' function did not match the function that the curriculum was assuming, so from this point on, I am picking up *Ryan*'s lecture from the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just for the lesson example\n",
    "import unicodedata\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "# natural language toolkit\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Paul Erdős and George Pólya are influential Hungarian mathematicians who contributed a lot to the field. \\nErdős's name contains the Hungarian letter 'ő' ('o' with double acute accent), but is often incorrectly written\\nas Erdos or Erdös either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = \"\"\"Paul Erdős and George Pólya are influential Hungarian mathematicians who contributed a lot to the field. \n",
    "Erdős's name contains the Hungarian letter 'ő' ('o' with double acute accent), but is often incorrectly written\n",
    "as Erdos or Erdös either by mistake or out of typographical necessity\"\"\"\n",
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematicians who contributed a lot to the field. \\nerdos's name contains the hungarian letter 'o' ('o' with double acute accent), but is often incorrectly written\\nas erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase and remove accented characters and any non-ASCII characters\n",
    "# Encode to ASCII, to convert special characters into ASCII \n",
    "# Decode from ASCII to UTF-8 so we have a normal Python string\n",
    "string = original.lower()\n",
    "string = unicodedata.normalize('NFKD', string)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')    \n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematicians who contributed a lot to the field \\nerdos's name contains the hungarian letter 'o' 'o' with double acute accent but is often incorrectly written\\nas erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any special characters and replace with an empty string\n",
    "string = re.sub(r\"[^a-z0-9'\\s]\", '', string)\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have lowercased everything and done all the ASCII encoding / decoding, time to Tokenize\n",
    "\n",
    "Hint for remembering:\n",
    "\n",
    "\"Yo!  That text is all little, and we've put the ASCII back into a string!\"\n",
    "\n",
    "\"Betta tokenize!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematicians who contributed a lot to the field \\nerdos ' s name contains the hungarian letter ' o ' ' o ' with double acute accent but is often incorrectly written\\nas erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can accomplish the above two step by using a tokenizer\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "string = tokenizer.tokenize(string, return_str=True)\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now's the time to either Stem or Lemmatize.  We stem if there's a ton of data because it's brute force, but takes up way less computer energy to do.  We Lemmatize data that can fit onto our laptops because while it's way more refined than stemming (lemmatized words are in the dictionary whereas stemmed words are not), it is more computationally intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('call', 'call', 'call')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming is a super basic way to get only the \"stem\" of a word\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "ps.stem('call'), ps.stem('called'), ps.stem('calling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paul',\n",
       " 'erdo',\n",
       " 'and',\n",
       " 'georg',\n",
       " 'polya',\n",
       " 'are',\n",
       " 'influenti',\n",
       " 'hungarian',\n",
       " 'mathematician',\n",
       " 'who',\n",
       " 'contribut',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'to',\n",
       " 'the',\n",
       " 'field',\n",
       " 'erdo',\n",
       " \"'\",\n",
       " 's',\n",
       " 'name',\n",
       " 'contain',\n",
       " 'the',\n",
       " 'hungarian',\n",
       " 'letter',\n",
       " \"'\",\n",
       " 'o',\n",
       " \"'\",\n",
       " \"'\",\n",
       " 'o',\n",
       " \"'\",\n",
       " 'with',\n",
       " 'doubl',\n",
       " 'acut',\n",
       " 'accent',\n",
       " 'but',\n",
       " 'is',\n",
       " 'often',\n",
       " 'incorrectli',\n",
       " 'written',\n",
       " 'as',\n",
       " 'erdo',\n",
       " 'or',\n",
       " 'erdo',\n",
       " 'either',\n",
       " 'by',\n",
       " 'mistak',\n",
       " 'or',\n",
       " 'out',\n",
       " 'of',\n",
       " 'typograph',\n",
       " 'necess']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems = [ps.stem(word) for word in string.split()]\n",
    "stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [wnl.lemmatize(word) for word in string.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paul',\n",
       " 'erdos',\n",
       " 'and',\n",
       " 'george',\n",
       " 'polya',\n",
       " 'are',\n",
       " 'influential',\n",
       " 'hungarian',\n",
       " 'mathematician',\n",
       " 'who',\n",
       " 'contributed',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'to',\n",
       " 'the',\n",
       " 'field',\n",
       " 'erdos',\n",
       " \"'\",\n",
       " 's',\n",
       " 'name',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'hungarian',\n",
       " 'letter',\n",
       " \"'\",\n",
       " 'o',\n",
       " \"'\",\n",
       " \"'\",\n",
       " 'o',\n",
       " \"'\",\n",
       " 'with',\n",
       " 'double',\n",
       " 'acute',\n",
       " 'accent',\n",
       " 'but',\n",
       " 'is',\n",
       " 'often',\n",
       " 'incorrectly',\n",
       " 'written',\n",
       " 'a',\n",
       " 'erdos',\n",
       " 'or',\n",
       " 'erdos',\n",
       " 'either',\n",
       " 'by',\n",
       " 'mistake',\n",
       " 'or',\n",
       " 'out',\n",
       " 'of',\n",
       " 'typographical',\n",
       " 'necessity']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list = stopwords.words('english')\n",
    "print(len(stopword_list))\n",
    "stopword_list[0:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paul',\n",
       " 'erdo',\n",
       " 'georg',\n",
       " 'polya',\n",
       " 'influenti',\n",
       " 'hungarian',\n",
       " 'mathematician',\n",
       " 'contribut',\n",
       " 'lot',\n",
       " 'field',\n",
       " 'erdo',\n",
       " \"'\",\n",
       " 'name',\n",
       " 'contain',\n",
       " 'hungarian',\n",
       " 'letter',\n",
       " \"'\",\n",
       " \"'\",\n",
       " \"'\",\n",
       " \"'\",\n",
       " 'doubl',\n",
       " 'acut',\n",
       " 'accent',\n",
       " 'often',\n",
       " 'incorrectli',\n",
       " 'written',\n",
       " 'erdo',\n",
       " 'erdo',\n",
       " 'either',\n",
       " 'mistak',\n",
       " 'typograph',\n",
       " 'necess']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_stems = [w for w in stems if w not in stopword_list]\n",
    "clean_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paul',\n",
       " 'erdos',\n",
       " 'george',\n",
       " 'polya',\n",
       " 'influential',\n",
       " 'hungarian',\n",
       " 'mathematician',\n",
       " 'contributed',\n",
       " 'lot',\n",
       " 'field',\n",
       " 'erdos',\n",
       " \"'\",\n",
       " 'name',\n",
       " 'contains',\n",
       " 'hungarian',\n",
       " 'letter',\n",
       " \"'\",\n",
       " \"'\",\n",
       " \"'\",\n",
       " \"'\",\n",
       " 'double',\n",
       " 'acute',\n",
       " 'accent',\n",
       " 'often',\n",
       " 'incorrectly',\n",
       " 'written',\n",
       " 'erdos',\n",
       " 'erdos',\n",
       " 'either',\n",
       " 'mistake',\n",
       " 'typographical',\n",
       " 'necessity']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_lemmas = [w for w in lemmas if w not in stopword_list]\n",
    "clean_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SO REMEMBER THE NLP PROCESS IN ORDER:\n",
    "\n",
    "- lowercase() all the things\n",
    "\n",
    "- remove special characters with re.sub and replace with empty string\n",
    "\n",
    "- tokenize your string\n",
    "\n",
    "- lemmatize (or stem if your corpus is gigantic)\n",
    "\n",
    "- remove your stopwords\n",
    "\n",
    "- keep the original text\n",
    "\n",
    "- write the original text and the transformed text to disk for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "- The end result of this exercise should be a file named prepare.py that defines the requested functions\n",
    "\n",
    "- In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "- It should lowercase everything\n",
    "\n",
    "- It should normalize the unicode characters\n",
    "\n",
    "- It should replace anything that is NOT a letter, number, whitespace, or single-quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\n",
    "\n",
    "def basic_clean(string):\n",
    "    \"\"\"\n",
    "    Function take a string, lowercase it, normalize it, and remove all non-ASCII characters\n",
    "    \"\"\"\n",
    "    \n",
    "    # to lowercase all the text:\n",
    "    string = string.lower()\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore') \n",
    "    \n",
    "    # to remove anything that is not a letter, number, whitespace, or single-quote:\n",
    "    string = re.sub(r\"[^a-zA-Z0-9'\\s]\", '', string)\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing out the (^^) function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kjadsfioeinf8vh84h8348 0e9u34j'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_clean(\"kjads;fioeinf8vh84h8348** 0-_e9u34j)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Define a function named 'tokenize' that takes in a string and tokenizes all the words in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    \"\"\"\n",
    "    Function to take in a string and break it down into discrete parts\n",
    "    \"\"\"\n",
    "    \n",
    "    # to lowercase all the text:\n",
    "    string = string.lower()\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore') \n",
    "    \n",
    "    # to remove anything that is not a letter, number, whitespace, or single-quote:\n",
    "    string = re.sub(r\"[^a-zA-Z0-9'\\s]\", '', string)\n",
    "\n",
    "    # applying the tokenizer object\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    string = tokenizer.tokenize(string, return_str=True)\n",
    "    \n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing out the (^^) function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kalieoinekndlapoivanwonpvo i hpwoihfpioypy028ty0'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"kalIEOINEknd;lapoivanwonpvo i hpwoihfpioY)*P$Y028ty0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) Define a function named 'stem' that accepts a string and returns it after stemming all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    \"\"\"\n",
    "    Function that takes in text and stems all the words down into their base forms\n",
    "    \"\"\"\n",
    "    \n",
    "    # to lowercase all the text:\n",
    "    string = string.lower()\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore') \n",
    "    \n",
    "    # to remove anything that is not a letter, number, whitespace, or single-quote:\n",
    "    string = re.sub(r\"[^a-zA-Z0-9'\\s]\", '', string)\n",
    "    \n",
    "    # applying the tokenizer object\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    string = tokenizer.tokenize(string, return_str=True)\n",
    "    \n",
    "    # stemming everything\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    words = [string]\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    string_stemmed = ' '.join(stems)\n",
    "\n",
    "    return string_stemmed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing out the (^^) function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there onc wa a man from nantucket'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem(\"There once was a man from Nantucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks out.  'Onc' and 'wa' are the stemmed 'words'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) Define a function called 'lemmatize' that accepts some text and returns it after the lemmatization of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "    Function that takes in text and stems all the words down into their base forms\n",
    "    \"\"\"\n",
    "    \n",
    "    # to lowercase all the text:\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore') \n",
    "    \n",
    "    # to remove anything that is not a letter, number, whitespace, or single-quote:\n",
    "    text = re.sub(r\"[^a-zA-Z0-9'\\s]\", '', text)\n",
    "    \n",
    "    # applying the tokenizer object\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    text = tokenizer.tokenize(text, return_str=True)\n",
    "    \n",
    "    # lemmatizing everything\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "    text_lemmatized = ' '.join(lemmas)\n",
    "\n",
    "    return text_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing out the (^^) function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'firecracker chicken or shrimp battered and deep fried sauteed in honey with yellow onion and jalapeno'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"Firecracker Chicken or Shrimp. Battered and deep fried sauteed in honey with yellow onions and jalapenos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks out.  The period after 'Shrimp' and the 's' being removed at the end of 'jalapenos' indicate it works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.) Define a function called 'remove_stopwords' that accepts some text and returns it after removing all the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# take a look at the first 20 words in the stopwords dictionary.  This is in English.  Tried in Spanish.  Same thing.\n",
    "\n",
    "stopword_list = stopwords.words('english')\n",
    "print(stopword_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, extra_words=\"\", exclude_words=\"\"):\n",
    "    \"\"\"\n",
    "    Function that takes in text and stems all the words down into their base forms\n",
    "    \"\"\"\n",
    "    \n",
    "    # to lowercase all the text:\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore') \n",
    "    \n",
    "    # to remove anything that is not a letter, number, whitespace, or single-quote:\n",
    "    text = re.sub(r\"[^a-zA-Z0-9'\\s]\", '', text)\n",
    "    \n",
    "    # applying the tokenizer object\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    text = tokenizer.tokenize(text, return_str=True)\n",
    "    \n",
    "    # lemmatizing everything\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "    text_lemmatized = ' '.join(lemmas)\n",
    "\n",
    "    # removing stopwords\n",
    "    words = text.split()\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    print(\"removed {} stopwords\".format(len(words) - len(filtered_words)))\n",
    "    print(\"***************\")\n",
    "    text_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return text_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test out the (^^) function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 5 stopwords\n",
      "***************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'firecracker chicken shrimp battered deep fried sauteed honey yellow onions jalapenos'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"Firecracker Chicken or Shrimp. Battered and deep fried sauteed in honey with yellow onions and jalapenos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 9 stopwords\n",
      "***************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"alright stop ' ' ruin\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"Alright, stop what you're doing because I'm about to ruin\", \"doing\", \"alright\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.) Define a function called 'prep_article' that takes in the dictionary representing an article and returns a dictionary that looks like thie:\n",
    "\n",
    "{\n",
    "\n",
    "    \"title\" : \"the original title\",\n",
    "    \n",
    "    \"original\" : original `#` the entire text in its original format\n",
    "    \n",
    "    \"stemmed\" : article_stemmed `#` all the text, stemmed\n",
    "    \n",
    "    \"lemmatized\" : article_lemmatized `#` all the text, lemmatized\n",
    "    \n",
    "    \"clean\" : article_without_stopwords `#` all the text lemmatized and all the stopwords removed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.) Define a function named 'prepare_article_data' that takes in the list of article dictionaries, applies the 'prep_article' function to each one, and returns the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df):\n",
    "    '''\n",
    "    This function takes in the news articles df and\n",
    "    returns the df with original columns plus cleaned\n",
    "    and lemmatized content without stopwords.\n",
    "    '''\n",
    "    # Do basic clean on article content\n",
    "    df = basic_clean(df, 'content')\n",
    "    \n",
    "    # Tokenize clean article content\n",
    "    df = tokenize(df, 'basic_clean')\n",
    "    \n",
    "    # Stem cleaned and tokenized article content\n",
    "    df = stem(df, 'clean_tokes')\n",
    "    \n",
    "    # Remove stopwords from Lemmatized article content\n",
    "    df = remove_stopwords(df, 'stemmed')\n",
    "    \n",
    "    # Lemmatize cleaned and tokenized article content\n",
    "    df = lemmatize(df, 'clean_tokes')\n",
    "    \n",
    "    # Remove stopwords from Lemmatized article content\n",
    "    df = remove_stopwords(df, 'lemmatized')\n",
    "    \n",
    "    return df[['topic', 'title', 'author', 'content', 'clean_stemmed', 'clean_lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
